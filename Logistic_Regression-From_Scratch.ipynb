{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAT5xMFHqGGT+WQYj4+BsI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fawzy-AI-Explorer/Fawzy-AI-Explorer/blob/main/Logistic_Regression-From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression From Scratct\n",
        "## Outline:\n",
        "- [Goals](#goals)\n",
        "- [Tools](#tools)\n",
        "- [Prediction && Accuracy](#pred_acc)\n",
        "- [Compute Cost](#cost)\n",
        "- [Gradient Descent](#grd_desc)\n",
        "- [Final Implementation](#final_implementation)"
      ],
      "metadata": {
        "id": "VbEKtzrMtk09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"goals\">Goals<a>\n",
        "In this toturial, we will:\n",
        "\n",
        "- Implement the logistic regression model.\n",
        "- Implement the gradient descent algorithm to train the model.\n",
        "- Implement the accuracy score to evaluate the model.<br><br>\n",
        "\n",
        "*All from scratch*"
      ],
      "metadata": {
        "id": "SqLxA5pAusjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"tools\">Tools<a>\n",
        "In this project, we will make use of:\n",
        "- math, This module provides access to the mathematical functions defined by the C standard\n",
        "- pandas, a Python library used for working with data sets\n",
        "- NumPy, a popular library for scientific computing\n",
        "- seaborn, a Python data visualization library based on matplotlib\n",
        "- Matplotlib, a popular library for plotting data"
      ],
      "metadata": {
        "id": "C88Qmwf0vCd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hThsZhi1suD2"
      },
      "outputs": [],
      "source": [
        "import math, copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"pred_acc\">Prediction && Accuracy<a>\n",
        "### Logistic Regression\n",
        "A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{1} $$\n",
        "\n",
        "  where\n",
        "\n",
        "  $g(z) = \\frac{1}{1+e^{-z}}\\tag{2}$\n",
        "\n",
        "<br><br>\n",
        "**Accuracy Score**: This is the proportion of correct predictions among all predictions made by the model.<br><br>\n",
        "$$Accuracy=\\frac{Number\\ of\\ Correct\\ Predictions}{Total\\ Number\\ of\\ Predictions}\\tag{3}$$<br>"
      ],
      "metadata": {
        "id": "vX9Rde2W4cZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid function implementation\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g (ndarray): sigmoid(z), with the same shape as z\n",
        "    \"\"\"\n",
        "    # apply the sigmoid function\n",
        "    g = 1 / (1 + np.exp(-z))\n",
        "    return g"
      ],
      "metadata": {
        "id": "h_Yd32c_4c-o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction function implemntation\n",
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      X (ndarray): Shape (m, n) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "    \"\"\"\n",
        "    # apply sigmiod and threshold\n",
        "    p = sigmoid(np.dot(w, X) + b)\n",
        "    return p"
      ],
      "metadata": {
        "id": "-GcRBoLi4nEv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's make another function that predicts the exact category"
      ],
      "metadata": {
        "id": "4cf_1JcR4rz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predicts the exact value for the class\n",
        "def predict_class(X, w, b):\n",
        "    \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      X (ndarray): Shape (m, n) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "    \"\"\"\n",
        "    # apply sigmiod and threshold\n",
        "    p = 1. if sigmoid(np.dot(X, w) + b) >= 0.5 else 0.\n",
        "    return p"
      ],
      "metadata": {
        "id": "iHCTAKZC4rIK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy function implementation\n",
        "def get_accuracy(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Returns the accuracy of the model\n",
        "    Args:\n",
        "    X (ndarray): Shape(m, n) examples with multiple features\n",
        "    y (ndarray): Shape (m,) the actual target values\n",
        "    w (ndarray): Shape (n,) model parameters\n",
        "    b (scalar) : model parameter\n",
        "\n",
        "    Returns:\n",
        "      accuracy (scalar): the accuracy of the model\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    correct = 0\n",
        "    # count the correct predictions\n",
        "    for i in range(m):\n",
        "      prediction = predict_class(X[i], w, b)\n",
        "      if prediction == y[i]:\n",
        "        correct += 1\n",
        "    # proportion of correct predictions among all predictions\n",
        "    accuracy = round(correct / m, 2)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "748JH2gq4xlZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"cost\">Compute Cost<a>\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = -\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}[y^{(i)} log(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) + (1 - {y}^{(i)})log(1 - f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))]\\tag{1}$$"
      ],
      "metadata": {
        "id": "vT62LBX_46by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cost function for logistic regression\n",
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for logistic regression.\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples and n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (m,)): model parameters\n",
        "      b (scalar)    : model parameter\n",
        "\n",
        "    Returns:\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    # examples and features\n",
        "    m, n = X.shape\n",
        "    total_cost = 0.\n",
        "\n",
        "    # compute the loss for each example\n",
        "    for i in range(m):\n",
        "      epsilon = 1e-9 # to avoid overflow\n",
        "      f_wb_i = predict(X[i], w, b) + epsilon\n",
        "      total_cost += (y[i] * np.log(f_wb_i) + (1 - y[i]) * np.log(1 - f_wb_i))\n",
        "\n",
        "    total_cost /= -m\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "DkZc-Is84zHZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"grd_desc\">Gradient Descent<a>\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "metadata": {
        "id": "P8QdBPy75E1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "let's start by computing the gradients"
      ],
      "metadata": {
        "id": "G4Ru5FRi5Hcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to compute the gradients\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    # number of examples\n",
        "    m, n = X.shape\n",
        "\n",
        "    # initial gradients\n",
        "    dj_dw = np.zeros((n, ))\n",
        "    dj_db = 0.\n",
        "\n",
        "    # compute the actual gradient for each parameter\n",
        "    for i in range(m):\n",
        "      f_wb_i = predict(X[i], w, b)\n",
        "      err = f_wb_i - y[i]\n",
        "      dj_dw += err * X[i]\n",
        "      dj_db += err\n",
        "\n",
        "    dj_dw /= m\n",
        "    dj_db /= m\n",
        "\n",
        "    return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "gzpf4mqS49vK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's apply the algorithm"
      ],
      "metadata": {
        "id": "_p9SwPcU5PZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply gradient descent algorithm\n",
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "    \"\"\"\n",
        "    # initialize some variables\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)\n",
        "    b = b_in\n",
        "\n",
        "    # apply the algorithm `num_iters` of iterations\n",
        "    for i in range(num_iters):\n",
        "      # compute gradient\n",
        "      dj_dw, dj_db = gradient_function(X, y , w, b)\n",
        "      # simultneous update\n",
        "      w = w - alpha * dj_dw\n",
        "      b = b - alpha * dj_db\n",
        "\n",
        "      # save the history\n",
        "      if i < 100000:\n",
        "        J_history.append(cost_function(X, y, w, b))\n",
        "\n",
        "      # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "      if i % math.ceil(num_iters / 10) == 0:\n",
        "        print(\"{:>8} {:>11.5e}\".format(i, cost_function(X, y, w, b)))\n",
        "\n",
        "    return w, b, J_history\n"
      ],
      "metadata": {
        "id": "kvKcURir5Mu6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"final_implementation\">Final Implementation<a>\n",
        "Now we will grab all this together to make the complete model that can be used easily later.<br><br>\n",
        "*NOTE*: we will rename some methods just to keep up with the original model"
      ],
      "metadata": {
        "id": "LDKTgHES5dHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a linear regressino class.\"\"\"\n",
        "\n",
        "\n",
        "class LogisticRegression():\n",
        "    \"\"\"Representation of a logistic regression model.\n",
        "    the model predicts categories from small number of possible outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=1e-3, n_iters=1000):\n",
        "      \"\"\"Initialize the linear regression model\n",
        "\n",
        "      Args:\n",
        "        learning_rate (scalar) : The number indicates the step in gradient descent.\n",
        "        n_iters (scalar) : The maximum number of passes over the training data.\n",
        "      \"\"\"\n",
        "      self.lr = learning_rate\n",
        "      self.n_iters = n_iters\n",
        "      self.weights = None\n",
        "      self.bias = None\n",
        "\n",
        "    def sigmoid(z):\n",
        "      \"\"\"\n",
        "      Compute the sigmoid of z\n",
        "\n",
        "      Args:\n",
        "          z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "      Returns:\n",
        "          g (ndarray): sigmoid(z), with the same shape as z\n",
        "      \"\"\"\n",
        "      # apply the sigmoid function\n",
        "      g = 1 / (1 + np.exp(-z))\n",
        "      return g"
      ],
      "metadata": {
        "id": "qSsBvCcH86JG"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}